{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa624f6",
   "metadata": {},
   "source": [
    "# Testing Deployed Models in Azure AI Foundry\n",
    "\n",
    "This notebook guides you through testing your deployed customer service AI model. You'll learn:\n",
    "1. Basic model testing\n",
    "2. Performance evaluation\n",
    "3. Load testing\n",
    "4. Error handling and validation\n",
    "5. Best practices for model testing\n",
    "\n",
    "## Prerequisites\n",
    "- Completed model deployment\n",
    "- Azure AI Foundry access\n",
    "- Required Python packages installed\n",
    "- Active model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.evaluation import TextEvaluator\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "import azure.monitor.opentelemetry._autoinstrument\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# Check environment variables\n",
    "required_vars = {\n",
    "    \"AZURE_SUBSCRIPTION_ID\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"AZURE_RESOURCE_GROUP\": os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "}\n",
    "\n",
    "missing_vars = [var for var, value in required_vars.items() if not value]\n",
    "if missing_vars:\n",
    "    print(\"× Missing required environment variables:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"  - {var}\")\n",
    "else:\n",
    "    print(\"✓ All required environment variables are set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4aa9b1",
   "metadata": {},
   "source": [
    "## Initialize Testing Environment\n",
    "First, let's set up our testing environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_test_environment():\n",
    "    \"\"\"Initialize the testing environment and client.\"\"\"\n",
    "    try:\n",
    "        # Initialize credentials\n",
    "        credential = DefaultAzureCredential()\n",
    "        print(\"✓ Successfully initialized DefaultAzureCredential\")\n",
    "        \n",
    "        # Create client\n",
    "        client = AIProjectClient(\n",
    "            subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "            resource_group=os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "            credential=credential\n",
    "        )\n",
    "        print(\"✓ Successfully initialized AIProjectClient\")\n",
    "        \n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"× Error initializing test environment: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Initialize client\n",
    "client = initialize_test_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567da819",
   "metadata": {},
   "source": [
    "## Basic Model Testing\n",
    "Let's start with basic functionality tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67adf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_functionality(deployment_name: str, test_cases: list):\n",
    "    \"\"\"Run basic functionality tests on the deployed model.\"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        for test_case in test_cases:\n",
    "            # Send request to model\n",
    "            response = client.deployments.invoke(\n",
    "                deployment_name=deployment_name,\n",
    "                input_data=test_case[\"input\"]\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            success = validate_response(response, test_case[\"expected\"])\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"test_case\": test_case[\"name\"],\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"output\": response,\n",
    "                \"expected\": test_case[\"expected\"],\n",
    "                \"success\": success\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\nTest Results:\")\n",
    "        print(df[[\"test_case\", \"success\"]].to_string())\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in basic testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"BMI calculation\",\n",
    "        \"input\": \"Calculate BMI for someone 5'9\" and 160 pounds\",\n",
    "        \"expected\": {\"type\": \"health_calculation\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Meal planning\",\n",
    "        \"input\": \"Create a meal plan for someone with diabetes\",\n",
    "        \"expected\": {\"type\": \"dietary_advice\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Dietary restrictions\",\n",
    "        \"input\": \"What foods should I avoid with celiac disease?\",\n",
    "        \"expected\": {\"type\": \"health_guidance\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run basic tests\n",
    "results = test_basic_functionality(\"customer-service-v1\", test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36c7eb",
   "metadata": {},
   "source": [
    "## Performance Testing\n",
    "Now let's evaluate the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_performance_test(deployment_name: str, requests_per_second: int, duration_seconds: int):\n",
    "    \"\"\"Run performance tests on the deployed model.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        end_time = start_time + duration_seconds\n",
    "        total_requests = requests_per_second * duration_seconds\n",
    "        \n",
    "        async def make_request():\n",
    "            try:\n",
    "                response = await client.deployments.invoke_async(\n",
    "                    deployment_name=deployment_name,\n",
    "                    input_data=\"Test request for performance evaluation\"\n",
    "                )\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"latency\": response.get(\"latency\", 0),\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "        \n",
    "        # Create tasks for concurrent requests\n",
    "        tasks = []\n",
    "        for _ in range(total_requests):\n",
    "            tasks.append(make_request())\n",
    "        \n",
    "        # Run requests\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Analyze results\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        print(\"\\nPerformance Test Results:\")\n",
    "        print(f\"Total Requests: {len(results)}\")\n",
    "        print(f\"Successful Requests: {df['success'].sum()}\")\n",
    "        print(f\"Failed Requests: {len(results) - df['success'].sum()}\")\n",
    "        print(f\"Average Latency: {df[df['success']]['latency'].mean():.2f}ms\")\n",
    "        print(f\"95th Percentile Latency: {df[df['success']]['latency'].quantile(0.95):.2f}ms\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in performance testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run performance test\n",
    "asyncio.run(run_performance_test(\n",
    "    deployment_name=\"customer-service-v1\",\n",
    "    requests_per_second=10,\n",
    "    duration_seconds=30\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ab894",
   "metadata": {},
   "source": [
    "## Load Testing\n",
    "Let's test how the model handles increasing load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_load_test(deployment_name: str, max_concurrent_requests: int):\n",
    "    \"\"\"Run load tests with increasing concurrent requests.\"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        \n",
    "        for concurrent_requests in range(1, max_concurrent_requests + 1, 5):\n",
    "            print(f\"\\nTesting with {concurrent_requests} concurrent requests...\")\n",
    "            \n",
    "            # Run concurrent requests\n",
    "            test_results = asyncio.run(run_performance_test(\n",
    "                deployment_name=deployment_name,\n",
    "                requests_per_second=concurrent_requests,\n",
    "                duration_seconds=10\n",
    "            ))\n",
    "            \n",
    "            # Analyze results\n",
    "            df = pd.DataFrame(test_results)\n",
    "            results.append({\n",
    "                \"concurrent_requests\": concurrent_requests,\n",
    "                \"success_rate\": df['success'].mean() * 100,\n",
    "                \"avg_latency\": df[df['success']]['latency'].mean(),\n",
    "                \"p95_latency\": df[df['success']]['latency'].quantile(0.95)\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"\\nLoad Test Results:\")\n",
    "        print(results_df.to_string())\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in load testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run load test\n",
    "load_results = run_load_test(\n",
    "    deployment_name=\"customer-service-v1\",\n",
    "    max_concurrent_requests=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa4dfa",
   "metadata": {},
   "source": [
    "## Error Handling Testing\n",
    "Test how the model handles various error conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb386daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_handling(deployment_name: str):\n",
    "    \"\"\"Test model's error handling capabilities.\"\"\"\n",
    "    try:\n",
    "        error_test_cases = [\n",
    "            {\n",
    "                \"name\": \"Empty input\",\n",
    "                \"input\": \"\",\n",
    "                \"expected_error\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Very long input\",\n",
    "                \"input\": \"a\" * 10000,\n",
    "                \"expected_error\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Invalid JSON\",\n",
    "                \"input\": \"{invalid_json:\",\n",
    "                \"expected_error\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Special characters\",\n",
    "                \"input\": \"!@#$%^&*()\",\n",
    "                \"expected_error\": False\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for test_case in error_test_cases:\n",
    "            try:\n",
    "                response = client.deployments.invoke(\n",
    "                    deployment_name=deployment_name,\n",
    "                    input_data=test_case[\"input\"]\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"test_case\": test_case[\"name\"],\n",
    "                    \"expected_error\": test_case[\"expected_error\"],\n",
    "                    \"actual_error\": False,\n",
    "                    \"handled_correctly\": not test_case[\"expected_error\"],\n",
    "                    \"response\": response\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"test_case\": test_case[\"name\"],\n",
    "                    \"expected_error\": test_case[\"expected_error\"],\n",
    "                    \"actual_error\": True,\n",
    "                    \"handled_correctly\": test_case[\"expected_error\"],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Display results\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\nError Handling Test Results:\")\n",
    "        print(df[[\"test_case\", \"handled_correctly\"]].to_string())\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in error handling testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run error handling tests\n",
    "error_results = test_error_handling(\"customer-service-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac6ede",
   "metadata": {},
   "source": [
    "## Response Validation\n",
    "Test the model's response format and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_responses(deployment_name: str):\n",
    "    \"\"\"Validate model responses for format and content.\"\"\"\n",
    "    try:\n",
    "        validation_test_cases = [\n",
    "            {\n",
    "                \"name\": \"Health calculation format\",\n",
    "                \"input\": \"Calculate BMI for someone 5'9\" and 160 pounds\",\n",
    "                \"validations\": [\n",
    "                    \"response_type\",\n",
    "                    \"calculation_result\",\n",
    "                    \"health_guidance\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Dietary advice content\",\n",
    "                \"input\": \"Create a meal plan for diabetes\",\n",
    "                \"validations\": [\n",
    "                    \"nutritional_info\",\n",
    "                    \"medical_disclaimer\",\n",
    "                    \"meal_components\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for test_case in validation_test_cases:\n",
    "            response = client.deployments.invoke(\n",
    "                deployment_name=deployment_name,\n",
    "                input_data=test_case[\"input\"]\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            validation_results = {}\n",
    "            for validation in test_case[\"validations\"]:\n",
    "                validation_results[validation] = validate_field(response, validation)\n",
    "            \n",
    "            results.append({\n",
    "                \"test_case\": test_case[\"name\"],\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"validations\": validation_results,\n",
    "                \"success\": all(validation_results.values())\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nValidation Test Results:\")\n",
    "        for result in results:\n",
    "            print(f\"\\nTest Case: {result['test_case']}\")\n",
    "            print(f\"Success: {'✓' if result['success'] else '×'}\")\n",
    "            print(\"Validation Results:\")\n",
    "            for validation, passed in result['validations'].items():\n",
    "                print(f\"  - {validation}: {'✓' if passed else '×'}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in validation testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def validate_field(response, field_type):\n",
    "    \"\"\"Validate specific fields in the response.\"\"\"\n",
    "    try:\n",
    "        if field_type == \"response_type\":\n",
    "            return isinstance(response.get(\"type\"), str)\n",
    "        elif field_type == \"calculation_result\":\n",
    "            result = response.get(\"bmi\")\n",
    "            return isinstance(result, (int, float)) and 0 <= result <= 100\n",
    "        elif field_type == \"health_guidance\":\n",
    "            guidance = response.get(\"guidance\")\n",
    "            return isinstance(guidance, str) and len(guidance) > 0\n",
    "        elif field_type == \"nutritional_info\":\n",
    "            info = response.get(\"nutrition\", {})\n",
    "            return isinstance(info, dict) and len(info) > 0\n",
    "        elif field_type == \"medical_disclaimer\":\n",
    "            disclaimer = response.get(\"disclaimer\")\n",
    "            return isinstance(disclaimer, str) and \"consult\" in disclaimer.lower()\n",
    "        elif field_type == \"meal_components\":\n",
    "            meals = response.get(\"meals\", [])\n",
    "            return isinstance(meals, list) and len(meals) > 0\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Run validation tests\n",
    "validation_results = validate_model_responses(\"customer-service-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63a953",
   "metadata": {},
   "source": [
    "## Best Practices for Model Testing\n",
    "\n",
    "1. **Comprehensive Testing Strategy**\n",
    "   - Unit tests for basic functionality\n",
    "   - Integration tests for end-to-end workflows\n",
    "   - Performance tests for scalability\n",
    "   - Load tests for stability\n",
    "   - Error handling tests for robustness\n",
    "\n",
    "2. **Test Data Management**\n",
    "   - Use diverse test cases\n",
    "   - Include edge cases\n",
    "   - Maintain test data versioning\n",
    "   - Regular test data updates\n",
    "\n",
    "3. **Performance Monitoring**\n",
    "   - Track response times\n",
    "   - Monitor error rates\n",
    "   - Analyze throughput\n",
    "   - Set up alerts\n",
    "\n",
    "4. **Documentation**\n",
    "   - Document test cases\n",
    "   - Record test results\n",
    "   - Maintain testing procedures\n",
    "   - Update documentation regularly\n",
    "\n",
    "5. **Continuous Testing**\n",
    "   - Automated testing pipeline\n",
    "   - Regular test execution\n",
    "   - Results tracking\n",
    "   - Continuous improvement"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
