{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bcbe6c",
   "metadata": {},
   "source": [
    "# Performance Metrics in Azure AI Evaluation\n",
    "    \n",
    "This notebook demonstrates how to set up and measure performance metrics for your customer service AI agent using Azure AI Evaluation.\n",
    "\n",
    "## Prerequisites\n",
    "- Azure subscription with access to Azure AI Foundry\n",
    "- Python environment with required packages installed\n",
    "- Completed the Introduction to Evaluation notebook\n",
    "- Working customer service agent implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Define custom evaluation metrics\n",
    "- Set up performance monitoring\n",
    "- Collect and analyze metrics\n",
    "- Optimize agent performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50091b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources import AIProjectClient\n",
    "from azure.ai.evaluation import EvaluationClient\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "\n",
    "# Initialize Azure clients\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(\n",
    "    subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    resource_group=os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "    credential=credential\n",
    ")\n",
    "evaluation_client = EvaluationClient(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf58e2",
   "metadata": {},
   "source": [
    "## Defining Performance Metrics\n",
    "\n",
    "Let's define key metrics for evaluating our customer service agent:\n",
    "1. Response Accuracy\n",
    "2. Response Time\n",
    "3. Task Completion Rate\n",
    "4. User Satisfaction Score\n",
    "5. Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea5647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "performance_metrics = {\n",
    "    \"response_accuracy\": {\n",
    "        \"type\": \"relevance\",\n",
    "        \"weight\": 0.3,\n",
    "        \"threshold\": 0.8\n",
    "    },\n",
    "    \"response_time\": {\n",
    "        \"type\": \"latency\",\n",
    "        \"weight\": 0.2,\n",
    "        \"threshold\": 2000  # milliseconds\n",
    "    },\n",
    "    \"task_completion\": {\n",
    "        \"type\": \"binary\",\n",
    "        \"weight\": 0.2\n",
    "    },\n",
    "    \"user_satisfaction\": {\n",
    "        \"type\": \"rating\",\n",
    "        \"weight\": 0.2,\n",
    "        \"scale\": [1, 5]\n",
    "    },\n",
    "    \"error_rate\": {\n",
    "        \"type\": \"error_count\",\n",
    "        \"weight\": 0.1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374998f",
   "metadata": {},
   "source": [
    "## Creating Test Cases\n",
    "\n",
    "We'll create a comprehensive set of test cases to evaluate our agent's performance across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"scenario\": \"Password Reset\",\n",
    "        \"input\": \"How do I reset my password?\",\n",
    "        \"expected_output\": \"To reset your password, click the 'Forgot Password' link, enter your email, and follow the instructions sent to your inbox.\",\n",
    "        \"expected_completion\": True\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Billing Inquiry\",\n",
    "        \"input\": \"When will I be charged for my subscription?\",\n",
    "        \"expected_output\": \"Billing occurs on the 1st of each month for your subscription.\",\n",
    "        \"expected_completion\": True\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Feature Information\",\n",
    "        \"input\": \"What features are included in the product?\",\n",
    "        \"expected_output\": \"The product includes cloud storage, synchronization capabilities, sharing features, and administrative controls.\",\n",
    "        \"expected_completion\": True\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06923d9",
   "metadata": {},
   "source": [
    "## Running Performance Evaluation\n",
    "\n",
    "Now let's create and run a performance evaluation using our defined metrics and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_performance():\n",
    "    try:\n",
    "        # Create evaluation run\n",
    "        evaluation = await evaluation_client.create_evaluation(\n",
    "            name=f\"customer-service-perf-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "            metrics=performance_metrics,\n",
    "            test_cases=test_cases\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = await evaluation.run()\n",
    "        \n",
    "        # Create performance report\n",
    "        report = {\n",
    "            \"overall_score\": results.overall_score,\n",
    "            \"metric_scores\": results.metric_scores,\n",
    "            \"response_times\": results.response_times,\n",
    "            \"error_count\": len(results.errors) if results.errors else 0\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run evaluation\n",
    "report = await evaluate_performance()\n",
    "\n",
    "# Display results\n",
    "if report:\n",
    "    print(\"Performance Evaluation Results:\")\n",
    "    print(f\"Overall Score: {report['overall_score']:.2f}\")\n",
    "    print(\"\n",
    "Metric Scores:\")\n",
    "    for metric, score in report['metric_scores'].items():\n",
    "        print(f\"{metric}: {score:.2f}\")\n",
    "    print(f\"\n",
    "Average Response Time: {sum(report['response_times'])/len(report['response_times']):.2f}ms\")\n",
    "    print(f\"Total Errors: {report['error_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90a62f",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "Let's create some visualizations to better understand our agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_results(report):\n",
    "    if not report:\n",
    "        print(\"No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\"metric\": metric, \"score\": score}\n",
    "        for metric, score in report['metric_scores'].items()\n",
    "    ])\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=metrics_df, x='metric', y='score')\n",
    "    plt.title('Performance Metrics Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "visualize_results(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b69e5",
   "metadata": {},
   "source": [
    "## Performance Optimization Recommendations\n",
    "\n",
    "Based on the evaluation results, here are some recommendations for improving agent performance:\n",
    "\n",
    "1. If response accuracy is low:\n",
    "   - Review and update training data\n",
    "   - Refine prompt engineering\n",
    "   - Consider using a more capable model\n",
    "\n",
    "2. If response times are high:\n",
    "   - Optimize code execution\n",
    "   - Consider caching frequent responses\n",
    "   - Review resource allocation\n",
    "\n",
    "3. If task completion rate is low:\n",
    "   - Analyze failed scenarios\n",
    "   - Implement better error handling\n",
    "   - Add support for edge cases\n",
    "\n",
    "4. If user satisfaction is low:\n",
    "   - Improve response quality\n",
    "   - Add more context awareness\n",
    "   - Implement feedback loop\n",
    "\n",
    "5. If error rate is high:\n",
    "   - Implement robust error handling\n",
    "   - Add input validation\n",
    "   - Improve edge case handling"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
