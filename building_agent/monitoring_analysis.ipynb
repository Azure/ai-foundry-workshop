{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29a9b52",
   "metadata": {},
   "source": [
    "# Monitoring and Analysis in Azure AI Evaluation\n",
    "    \n",
    "This notebook demonstrates how to set up monitoring and analyze evaluation results for your customer service AI agent.\n",
    "\n",
    "## Prerequisites\n",
    "- Azure subscription with access to Azure AI Foundry\n",
    "- Python environment with required packages installed\n",
    "- Completed the Performance Metrics notebook\n",
    "- Working customer service agent implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up real-time monitoring\n",
    "- Configure alerts and notifications\n",
    "- Analyze evaluation trends\n",
    "- Create performance dashboards\n",
    "- Implement continuous improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources import AIProjectClient\n",
    "from azure.ai.evaluation import EvaluationClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "\n",
    "# Initialize Azure clients\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(\n",
    "    subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    resource_group=os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "    credential=credential\n",
    ")\n",
    "evaluation_client = EvaluationClient(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3e710",
   "metadata": {},
   "source": [
    "## Setting Up Monitoring\n",
    "\n",
    "Let's configure real-time monitoring for our customer service agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_monitoring():\n",
    "    '''Configure monitoring settings for the evaluation system.'''\n",
    "    try:\n",
    "        monitoring_config = {\n",
    "            \"metrics\": {\n",
    "                \"collection_interval\": \"1m\",\n",
    "                \"retention_days\": 30,\n",
    "                \"metrics_to_track\": [\n",
    "                    \"response_time\",\n",
    "                    \"accuracy_score\",\n",
    "                    \"user_satisfaction\",\n",
    "                    \"error_rate\",\n",
    "                    \"completion_rate\"\n",
    "                ]\n",
    "            },\n",
    "            \"alerts\": {\n",
    "                \"error_rate_threshold\": 0.2,\n",
    "                \"response_time_threshold_ms\": 2000,\n",
    "                \"accuracy_threshold\": 0.8,\n",
    "                \"notification_channels\": [\"email\"]\n",
    "            },\n",
    "            \"dashboard\": {\n",
    "                \"refresh_interval\": \"5m\",\n",
    "                \"widgets\": [\n",
    "                    \"performance_metrics\",\n",
    "                    \"error_tracking\",\n",
    "                    \"user_satisfaction\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Apply monitoring configuration\n",
    "        await evaluation_client.configure_monitoring(monitoring_config)\n",
    "        print(\"✓ Monitoring configuration applied successfully\")\n",
    "        return monitoring_config\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up monitoring: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Set up monitoring\n",
    "monitoring_config = await setup_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4375f",
   "metadata": {},
   "source": [
    "## Real-time Monitoring\n",
    "\n",
    "Now let's implement real-time monitoring of our agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def monitor_performance(duration_minutes=5):\n",
    "    '''Monitor agent performance in real-time.'''\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        end_time = start_time + timedelta(minutes=duration_minutes)\n",
    "        metrics_data = []\n",
    "        \n",
    "        print(f\"Starting performance monitoring for {duration_minutes} minutes...\")\n",
    "        \n",
    "        while datetime.now() < end_time:\n",
    "            # Collect current metrics\n",
    "            current_metrics = await evaluation_client.get_current_metrics()\n",
    "            \n",
    "            metrics_data.append({\n",
    "                'timestamp': datetime.now(),\n",
    "                'response_time': current_metrics.get('response_time', 0),\n",
    "                'accuracy': current_metrics.get('accuracy_score', 0),\n",
    "                'error_rate': current_metrics.get('error_rate', 0),\n",
    "                'satisfaction': current_metrics.get('user_satisfaction', 0)\n",
    "            })\n",
    "            \n",
    "            # Check for alerts\n",
    "            if current_metrics.get('error_rate', 0) > monitoring_config['alerts']['error_rate_threshold']:\n",
    "                print(f\"⚠️ Alert: High error rate detected: {current_metrics['error_rate']:.2f}\")\n",
    "            \n",
    "            await asyncio.sleep(60)  # Wait for 1 minute\n",
    "            \n",
    "        return pd.DataFrame(metrics_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Monitoring error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run monitoring for 5 minutes\n",
    "metrics_df = await monitor_performance(duration_minutes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb9d55",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the collected metrics and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(metrics_df):\n",
    "    '''Analyze and visualize performance metrics.'''\n",
    "    if metrics_df is None or len(metrics_df) == 0:\n",
    "        print(\"No data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Create performance dashboard\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Response Time Trend\n",
    "    sns.lineplot(data=metrics_df, x='timestamp', y='response_time', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Response Time Trend')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Accuracy Trend\n",
    "    sns.lineplot(data=metrics_df, x='timestamp', y='accuracy', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Accuracy Trend')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Error Rate\n",
    "    sns.lineplot(data=metrics_df, x='timestamp', y='error_rate', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Error Rate Trend')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # User Satisfaction\n",
    "    sns.lineplot(data=metrics_df, x='timestamp', y='satisfaction', ax=axes[1,1])\n",
    "    axes[1,1].set_title('User Satisfaction Trend')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = {\n",
    "        'avg_response_time': metrics_df['response_time'].mean(),\n",
    "        'avg_accuracy': metrics_df['accuracy'].mean(),\n",
    "        'avg_error_rate': metrics_df['error_rate'].mean(),\n",
    "        'avg_satisfaction': metrics_df['satisfaction'].mean()\n",
    "    }\n",
    "    \n",
    "    print(\"\n",
    "Performance Summary:\")\n",
    "    for metric, value in summary.items():\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "# Analyze collected metrics\n",
    "analyze_performance(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d8d33",
   "metadata": {},
   "source": [
    "## Continuous Improvement\n",
    "\n",
    "Based on the monitoring and analysis results, let's implement some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_improvement_recommendations(metrics_df):\n",
    "    '''Generate recommendations based on performance analysis.'''\n",
    "    if metrics_df is None or len(metrics_df) == 0:\n",
    "        return \"No data available for recommendations\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Analyze response time\n",
    "    avg_response_time = metrics_df['response_time'].mean()\n",
    "    if avg_response_time > 1000:  # If average response time > 1 second\n",
    "        recommendations.append(\n",
    "            \"Response Time: Consider implementing caching or optimizing model inference\"\n",
    "        )\n",
    "    \n",
    "    # Analyze accuracy\n",
    "    avg_accuracy = metrics_df['accuracy'].mean()\n",
    "    if avg_accuracy < 0.9:\n",
    "        recommendations.append(\n",
    "            \"Accuracy: Review training data and consider model fine-tuning\"\n",
    "        )\n",
    "    \n",
    "    # Analyze error rate\n",
    "    avg_error_rate = metrics_df['error_rate'].mean()\n",
    "    if avg_error_rate > 0.1:\n",
    "        recommendations.append(\n",
    "            \"Error Rate: Implement better error handling and edge case detection\"\n",
    "        )\n",
    "    \n",
    "    # Analyze user satisfaction\n",
    "    avg_satisfaction = metrics_df['satisfaction'].mean()\n",
    "    if avg_satisfaction < 4.0:\n",
    "        recommendations.append(\n",
    "            \"User Satisfaction: Review user feedback and improve response quality\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate improvement recommendations\n",
    "recommendations = generate_improvement_recommendations(metrics_df)\n",
    "print(\"Improvement Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
