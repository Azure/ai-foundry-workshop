{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f1ec2d",
   "metadata": {},
   "source": [
    "# Introduction to Azure AI Evaluation\n",
    "    \n",
    "This notebook provides a hands-on introduction to evaluating AI models and agents using Azure AI Evaluation.\n",
    "\n",
    "## Prerequisites\n",
    "- Azure subscription with access to Azure AI Foundry\n",
    "- Python environment with required packages installed\n",
    "- Basic understanding of AI models and agents\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Azure AI Evaluation capabilities\n",
    "- Learn to set up evaluation metrics\n",
    "- Practice basic evaluation scenarios\n",
    "- Analyze evaluation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b39aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources import AIProjectClient\n",
    "from azure.ai.evaluation import EvaluationClient\n",
    "\n",
    "# Initialize credentials and clients\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(\n",
    "    subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    resource_group=os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "evaluation_client = EvaluationClient(credential=credential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33ce0f",
   "metadata": {},
   "source": [
    "## Basic Evaluation Setup\n",
    "\n",
    "Let's start with a simple evaluation scenario for our customer service agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60873d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    \"response_relevance\": {\n",
    "        \"type\": \"relevance\",\n",
    "        \"weight\": 0.4\n",
    "    },\n",
    "    \"response_accuracy\": {\n",
    "        \"type\": \"exact_match\",\n",
    "        \"weight\": 0.3\n",
    "    },\n",
    "    \"response_time\": {\n",
    "        \"type\": \"latency\",\n",
    "        \"weight\": 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": \"How do I reset my password?\",\n",
    "        \"expected_output\": \"To reset your password, click the 'Forgot Password' link, enter your email, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the product features?\",\n",
    "        \"expected_output\": \"Our product includes cloud storage, synchronization capabilities, sharing features, and administrative controls.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f8665",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "Now let's run some basic evaluations using our metrics and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0664819",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation():\n",
    "    try:\n",
    "        # Create evaluation run\n",
    "        evaluation = await evaluation_client.create_evaluation(\n",
    "            name=\"customer-service-basic-eval\",\n",
    "            metrics=evaluation_metrics,\n",
    "            test_cases=test_cases\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = await evaluation.run()\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Evaluation Results:\")\n",
    "        print(f\"Overall Score: {results.overall_score}\")\n",
    "        print(\"\n",
    "Metric Scores:\")\n",
    "        for metric, score in results.metric_scores.items():\n",
    "            print(f\"{metric}: {score}\")\n",
    "            \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run evaluation\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd1152",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "Let's look at how to interpret and analyze the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3be70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    performance_summary = {\n",
    "        \"total_tests\": len(test_cases),\n",
    "        \"successful_tests\": sum(1 for score in results.test_scores if score > 0.8),\n",
    "        \"average_response_time\": sum(results.response_times) / len(results.response_times)\n",
    "    }\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"Performance Summary:\")\n",
    "    print(f\"Total Tests: {performance_summary['total_tests']}\")\n",
    "    print(f\"Successful Tests: {performance_summary['successful_tests']}\")\n",
    "    print(f\"Success Rate: {(performance_summary['successful_tests'] / performance_summary['total_tests']) * 100:.2f}%\")\n",
    "    print(f\"Average Response Time: {performance_summary['average_response_time']:.2f}ms\")\n",
    "\n",
    "# Analyze the results\n",
    "analyze_results(await run_evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b0c57",
   "metadata": {},
   "source": [
    "## Error Handling and Best Practices\n",
    "\n",
    "Important considerations when working with Azure AI Evaluation:\n",
    "\n",
    "1. Always validate your metrics configuration\n",
    "2. Use appropriate test case sizes\n",
    "3. Monitor evaluation performance\n",
    "4. Handle timeouts and errors gracefully\n",
    "5. Store and version your evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_evaluation_config(metrics, test_cases):\n",
    "    '''Validate evaluation configuration.'''\n",
    "    try:\n",
    "        # Check metric weights sum to 1\n",
    "        total_weight = sum(metric[\"weight\"] for metric in metrics.values())\n",
    "        assert abs(total_weight - 1.0) < 0.001, \"Metric weights must sum to 1\"\n",
    "        \n",
    "        # Validate test cases\n",
    "        for test_case in test_cases:\n",
    "            assert \"input\" in test_case, \"Test case missing input\"\n",
    "            assert \"expected_output\" in test_case, \"Test case missing expected output\"\n",
    "        \n",
    "        print(\"âœ“ Evaluation configuration is valid\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Configuration error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Validate our configuration\n",
    "validate_evaluation_config(evaluation_metrics, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00982dc2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of Azure AI Evaluation, you can:\n",
    "1. Create more comprehensive evaluation metrics\n",
    "2. Design larger test case sets\n",
    "3. Implement continuous evaluation\n",
    "4. Set up automated evaluation pipelines\n",
    "5. Track evaluation results over time"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
