{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927cb1d7",
   "metadata": {},
   "source": [
    "# Testing Deployed Models in Azure AI Foundry\n",
    "\n",
    "This notebook guides you through testing your deployed customer service AI model. You'll learn:\n",
    "1. Basic model testing\n",
    "2. Performance evaluation\n",
    "3. Load testing\n",
    "4. Error handling and validation\n",
    "5. Best practices for model testing\n",
    "\n",
    "## Prerequisites\n",
    "- Completed model deployment\n",
    "- Azure AI Foundry access\n",
    "- Required Python packages installed\n",
    "- Active model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources import AIProjectClient\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# Check environment variables\n",
    "required_vars = {\n",
    "    \"AZURE_SUBSCRIPTION_ID\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"AZURE_RESOURCE_GROUP\": os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "}\n",
    "\n",
    "missing_vars = [var for var, value in required_vars.items() if not value]\n",
    "if missing_vars:\n",
    "    print(\"× Missing required environment variables:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"  - {var}\")\n",
    "else:\n",
    "    print(\"✓ All required environment variables are set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06dec3a",
   "metadata": {},
   "source": [
    "## Initialize Testing Environment\n",
    "First, let's set up our testing environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_test_environment():\n",
    "    \"\"\"Initialize the testing environment and client.\"\"\"\n",
    "    try:\n",
    "        # Initialize credentials\n",
    "        credential = DefaultAzureCredential()\n",
    "        print(\"✓ Successfully initialized DefaultAzureCredential\")\n",
    "        \n",
    "        # Create client\n",
    "        client = AIProjectClient(\n",
    "            subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "            resource_group=os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "            credential=credential\n",
    "        )\n",
    "        print(\"✓ Successfully initialized AIProjectClient\")\n",
    "        \n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"× Error initializing test environment: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Initialize client\n",
    "client = initialize_test_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b8b11",
   "metadata": {},
   "source": [
    "## Basic Model Testing\n",
    "Let's start with basic functionality tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_functionality(deployment_name: str, test_cases: list):\n",
    "    \"\"\"Run basic functionality tests on the deployed model.\"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        for test_case in test_cases:\n",
    "            # Send request to model\n",
    "            response = client.deployments.invoke(\n",
    "                deployment_name=deployment_name,\n",
    "                input_data=test_case[\"input\"]\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            success = validate_response(response, test_case[\"expected\"])\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"test_case\": test_case[\"name\"],\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"output\": response,\n",
    "                \"expected\": test_case[\"expected\"],\n",
    "                \"success\": success\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\nTest Results:\")\n",
    "        print(df[[\"test_case\", \"success\"]].to_string())\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in basic testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Basic greeting\",\n",
    "        \"input\": \"Hello, how can you help me today?\",\n",
    "        \"expected\": {\"type\": \"greeting\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Product inquiry\",\n",
    "        \"input\": \"What's the price of your basic plan?\",\n",
    "        \"expected\": {\"type\": \"product_info\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Support request\",\n",
    "        \"input\": \"I'm having trouble logging in\",\n",
    "        \"expected\": {\"type\": \"support\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run basic tests\n",
    "results = test_basic_functionality(\"customer-service-v1\", test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20788978",
   "metadata": {},
   "source": [
    "## Performance Testing\n",
    "Now let's evaluate the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_performance_test(deployment_name: str, requests_per_second: int, duration_seconds: int):\n",
    "    \"\"\"Run performance tests on the deployed model.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        end_time = start_time + duration_seconds\n",
    "        total_requests = requests_per_second * duration_seconds\n",
    "        \n",
    "        async def make_request():\n",
    "            try:\n",
    "                response = await client.deployments.invoke_async(\n",
    "                    deployment_name=deployment_name,\n",
    "                    input_data=\"Test request for performance evaluation\"\n",
    "                )\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"latency\": response.get(\"latency\", 0),\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "        \n",
    "        # Create tasks for concurrent requests\n",
    "        tasks = []\n",
    "        for _ in range(total_requests):\n",
    "            tasks.append(make_request())\n",
    "        \n",
    "        # Run requests\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Analyze results\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        print(\"\\nPerformance Test Results:\")\n",
    "        print(f\"Total Requests: {len(results)}\")\n",
    "        print(f\"Successful Requests: {df['success'].sum()}\")\n",
    "        print(f\"Failed Requests: {len(results) - df['success'].sum()}\")\n",
    "        print(f\"Average Latency: {df[df['success']]['latency'].mean():.2f}ms\")\n",
    "        print(f\"95th Percentile Latency: {df[df['success']]['latency'].quantile(0.95):.2f}ms\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in performance testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run performance test\n",
    "asyncio.run(run_performance_test(\n",
    "    deployment_name=\"customer-service-v1\",\n",
    "    requests_per_second=10,\n",
    "    duration_seconds=30\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30bb48",
   "metadata": {},
   "source": [
    "## Load Testing\n",
    "Let's test how the model handles increasing load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_load_test(deployment_name: str, max_concurrent_requests: int):\n",
    "    \"\"\"Run load tests with increasing concurrent requests.\"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        \n",
    "        for concurrent_requests in range(1, max_concurrent_requests + 1, 5):\n",
    "            print(f\"\\nTesting with {concurrent_requests} concurrent requests...\")\n",
    "            \n",
    "            # Run concurrent requests\n",
    "            test_results = asyncio.run(run_performance_test(\n",
    "                deployment_name=deployment_name,\n",
    "                requests_per_second=concurrent_requests,\n",
    "                duration_seconds=10\n",
    "            ))\n",
    "            \n",
    "            # Analyze results\n",
    "            df = pd.DataFrame(test_results)\n",
    "            results.append({\n",
    "                \"concurrent_requests\": concurrent_requests,\n",
    "                \"success_rate\": df['success'].mean() * 100,\n",
    "                \"avg_latency\": df[df['success']]['latency'].mean(),\n",
    "                \"p95_latency\": df[df['success']]['latency'].quantile(0.95)\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"\\nLoad Test Results:\")\n",
    "        print(results_df.to_string())\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in load testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run load test\n",
    "load_results = run_load_test(\n",
    "    deployment_name=\"customer-service-v1\",\n",
    "    max_concurrent_requests=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30f941",
   "metadata": {},
   "source": [
    "## Error Handling Testing\n",
    "Test how the model handles various error conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_handling(deployment_name: str):\n",
    "    \"\"\"Test model's error handling capabilities.\"\"\"\n",
    "    try:\n",
    "        error_test_cases = [\n",
    "            {\n",
    "                \"name\": \"Empty input\",\n",
    "                \"input\": \"\",\n",
    "                \"expected_error\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Very long input\",\n",
    "                \"input\": \"a\" * 10000,\n",
    "                \"expected_error\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Invalid JSON\",\n",
    "                \"input\": \"{invalid_json:\",\n",
    "                \"expected_error\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Special characters\",\n",
    "                \"input\": \"!@#$%^&*()\",\n",
    "                \"expected_error\": False\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for test_case in error_test_cases:\n",
    "            try:\n",
    "                response = client.deployments.invoke(\n",
    "                    deployment_name=deployment_name,\n",
    "                    input_data=test_case[\"input\"]\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"test_case\": test_case[\"name\"],\n",
    "                    \"expected_error\": test_case[\"expected_error\"],\n",
    "                    \"actual_error\": False,\n",
    "                    \"handled_correctly\": not test_case[\"expected_error\"],\n",
    "                    \"response\": response\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"test_case\": test_case[\"name\"],\n",
    "                    \"expected_error\": test_case[\"expected_error\"],\n",
    "                    \"actual_error\": True,\n",
    "                    \"handled_correctly\": test_case[\"expected_error\"],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Display results\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\nError Handling Test Results:\")\n",
    "        print(df[[\"test_case\", \"handled_correctly\"]].to_string())\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in error handling testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run error handling tests\n",
    "error_results = test_error_handling(\"customer-service-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85153d8b",
   "metadata": {},
   "source": [
    "## Response Validation\n",
    "Test the model's response format and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_responses(deployment_name: str):\n",
    "    \"\"\"Validate model responses for format and content.\"\"\"\n",
    "    try:\n",
    "        validation_test_cases = [\n",
    "            {\n",
    "                \"name\": \"Response format\",\n",
    "                \"input\": \"What are your business hours?\",\n",
    "                \"validations\": [\n",
    "                    \"response_type\",\n",
    "                    \"confidence_score\",\n",
    "                    \"response_text\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Response content\",\n",
    "                \"input\": \"I need technical support\",\n",
    "                \"validations\": [\n",
    "                    \"relevant_keywords\",\n",
    "                    \"sentiment_analysis\",\n",
    "                    \"action_items\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for test_case in validation_test_cases:\n",
    "            response = client.deployments.invoke(\n",
    "                deployment_name=deployment_name,\n",
    "                input_data=test_case[\"input\"]\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            validation_results = {}\n",
    "            for validation in test_case[\"validations\"]:\n",
    "                validation_results[validation] = validate_field(response, validation)\n",
    "            \n",
    "            results.append({\n",
    "                \"test_case\": test_case[\"name\"],\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"validations\": validation_results,\n",
    "                \"success\": all(validation_results.values())\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nValidation Test Results:\")\n",
    "        for result in results:\n",
    "            print(f\"\\nTest Case: {result['test_case']}\")\n",
    "            print(f\"Success: {'✓' if result['success'] else '×'}\")\n",
    "            print(\"Validation Results:\")\n",
    "            for validation, passed in result['validations'].items():\n",
    "                print(f\"  - {validation}: {'✓' if passed else '×'}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"× Error in validation testing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def validate_field(response, field_type):\n",
    "    \"\"\"Validate specific fields in the response.\"\"\"\n",
    "    try:\n",
    "        if field_type == \"response_type\":\n",
    "            return isinstance(response.get(\"type\"), str)\n",
    "        elif field_type == \"confidence_score\":\n",
    "            score = response.get(\"confidence\")\n",
    "            return isinstance(score, (int, float)) and 0 <= score <= 1\n",
    "        elif field_type == \"response_text\":\n",
    "            return isinstance(response.get(\"text\"), str)\n",
    "        elif field_type == \"relevant_keywords\":\n",
    "            keywords = response.get(\"keywords\", [])\n",
    "            return isinstance(keywords, list) and len(keywords) > 0\n",
    "        elif field_type == \"sentiment_analysis\":\n",
    "            sentiment = response.get(\"sentiment\")\n",
    "            return isinstance(sentiment, (str, float))\n",
    "        elif field_type == \"action_items\":\n",
    "            actions = response.get(\"actions\", [])\n",
    "            return isinstance(actions, list)\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Run validation tests\n",
    "validation_results = validate_model_responses(\"customer-service-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89633a",
   "metadata": {},
   "source": [
    "## Best Practices for Model Testing\n",
    "\n",
    "1. **Comprehensive Testing Strategy**\n",
    "   - Unit tests for basic functionality\n",
    "   - Integration tests for end-to-end workflows\n",
    "   - Performance tests for scalability\n",
    "   - Load tests for stability\n",
    "   - Error handling tests for robustness\n",
    "\n",
    "2. **Test Data Management**\n",
    "   - Use diverse test cases\n",
    "   - Include edge cases\n",
    "   - Maintain test data versioning\n",
    "   - Regular test data updates\n",
    "\n",
    "3. **Performance Monitoring**\n",
    "   - Track response times\n",
    "   - Monitor error rates\n",
    "   - Analyze throughput\n",
    "   - Set up alerts\n",
    "\n",
    "4. **Documentation**\n",
    "   - Document test cases\n",
    "   - Record test results\n",
    "   - Maintain testing procedures\n",
    "   - Update documentation regularly\n",
    "\n",
    "5. **Continuous Testing**\n",
    "   - Automated testing pipeline\n",
    "   - Regular test execution\n",
    "   - Results tracking\n",
    "   - Continuous improvement"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
